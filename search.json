[
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "Sampling Methods for Uncertainty Quantification",
    "section": "Instructor information",
    "text": "Instructor information\n\nInstructor: Ali Siahkoohi\n\n\nEmail: alisk@ucf.edu\nOffice hours: Wednesdays 11–12pm.\n\n\n\nSee webcourses for the office hour location."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Sampling Methods for Uncertainty Quantification",
    "section": "Description",
    "text": "Description\nThis special topics course introduces modern computational sampling methods for uncertainty quantification in scientific computing and engineering applications. Topics include mathematical and computational principles of classical and contemporary sampling approaches with emphasis on understanding the inner workings of deep generative models, variational inference, and simulation-based inference methods. Students implement sampling algorithms and apply them to uncertainty quantification problems through hands-on programming assignments."
  },
  {
    "objectID": "index.html#why-take-this-course",
    "href": "index.html#why-take-this-course",
    "title": "Sampling Methods for Uncertainty Quantification",
    "section": "Why take this course?",
    "text": "Why take this course?\nAI models have recently driven significant advances in various science and engineering domains, yet critical reliability concerns remain: models produce unreliable predictions, lack quantifiable safety guarantees, and their theoretical foundations remain opaque to many practitioners. Current approaches focused on scaling and post-hoc validation cannot systematically address these issues. In this course, you will learn modern uncertainty quantification techniques that not only can be applied widely in various science and engineering domains but also hold the key to designing more reliable AI models. You will go beyond treating generative models, which are key components of modern sampling and uncertainty quantification techniques, as black boxes, understanding and implementing the core mathematical principles that make them work. Through five hands-on programming assignments, you will build these algorithms from the ground up, gaining the deep foundational expertise needed to adapt, debug, and innovate in this rapidly evolving field."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Sampling Methods for Uncertainty Quantification",
    "section": "Prerequisites",
    "text": "Prerequisites\nStudents should have background in probability theory, linear algebra, and programming. Familiarity with deep learning basics is recommended but not required."
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Sampling Methods for Uncertainty Quantification",
    "section": "Textbooks",
    "text": "Textbooks\nThere is no required textbook for the class. A list of recommended papers will be provided during the course."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Sampling Methods for Uncertainty Quantification",
    "section": "Grading",
    "text": "Grading\n\nProgramming assignments 60% (5 assignments × 12% each)\nPaper presentation 25%\nLecture scribing 15% (number of scribing assignments will depend on total class enrollment)\nThere will be 2% extra credit for submitting teaching evaluation, if more than 80% of students submitted the evaluation"
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "Sampling Methods for Uncertainty Quantification",
    "section": "Course schedule",
    "text": "Course schedule\nCourse material will be posted on the website before each lecture. The instructor reserves the right to alter this schedule as needed.\n\n\n\nWeek\nModule\nLecture Topics\nResources\nAssignments/Notes\n\n\n\n\nWeek 1-2\nModule 1: Foundations\nUncertainty quantification in scientific computing Review of probability Monte Carlo integration and rejection sampling\nWhat is UQ Rethinking Uncertainty Information Adequacy Illusion You’re Confidently Wrong  Confidence Misleads Supervised OOD Detection  Mathematics for Machine Learning\nMLK Day - No Class on Jan 19 Scribing instructions and template (pdf, zip) Notes on Probability Notes on Linear Algebra Notes on Norms Notes on Calculus Notes on Linear/Quadratic Gradients Notes on Max and Argmax Rejection Sampling Notes Assignment instructions Assignment 1 a1.zip\n\n\nWeek 3-4\nModule 2: MCMC\nImportance sampling and introduction to MCMC: Metropolis-Hastings Gibbs sampling and convergence diagnostics Gradient-based MCMC (Langevin dynamics and MALA)  Hamiltonian Monte Carlo fundamentals\nHandbook of Markov Chain Monte Carlo Understanding the Metropolis-Hastings Algorithm Explaining the Gibbs Sampler Markov Chain Monte Carlo Lecture Notes MCMC Slides On Thinning of Chains in MCMC Practical Markov Chain Monte Carlo Stochastic Gradient Langevin Dynamics  Preconditioned SGLD  Hamiltonian Monte Carlo A Conceptual Introduction to Hamiltonian Monte Carlo  MCMC Interactive Gallery\nAssignment 1 due  Assignment 2 a2.zip\n\n\nWeek 5-7\nModule 3: Variational Inference\nIntroduction to variational inference and KL divergenceReparameterization and the ELBOStochastic, non-amortized, and amortized variational inferenceStein Variational Gradient Descent (SVGD) and particle-based methodsVI vs MCMC: tradeoffs, diagnostics, and hybrid methods\nVI Review for Statisticians Advances in VI Auto-Encoding Variational Bayes Monte Carlo Gradient Estimation Reliable Amortized VI Stein Variational Gradient Descent VI with Normalizing Flows Yes, but Did It Work? Evaluating VI Pareto Smoothed Importance Sampling Importance Weighted Autoencoders Practical Posterior Error Bounds from VI Objectives\nAssignment 2 dueAssignment 3 a3.zip\n\n\nWeek 8-9\nModule 4: Deep Generative Models\nNormalizing flows: theory and architecturesVariational autoencoders and amortized inferenceScore matching and diffusion modelsFlow matching and continuous normalizing flows\n\nAssignment 4Assignment 3 due\n\n\nWeek 10-11\nModule 5: Simulation-Based Inference\nLikelihood-free inference and Approximate Bayesian Computation (ABC)Neural posterior estimation and neural likelihood estimationSequential methods and active learningPractical implementation and diagnostic tools\n\nAssignment 5Assignment 4 due\n\n\nWeek 12-13\nInstructor-Guided Paper Discussions\nInstructor curates and leads discussions on 4-6 recent research papers\n\nAssignment 5 due\n\n\nWeek 14-15\nStudent Paper Presentations\nStudent presentations on research papers (selected from curated list or instructor-approved)Critical analysis and discussion of recent developmentsAdvanced topics and cutting-edge developments in sampling methodology\n\nPaper presentations"
  }
]